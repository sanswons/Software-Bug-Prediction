{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/Horizon', '../data/TensorFlow-Examples', '../data/text_classification', '../data/Detectron', '../data/neural-style']\n"
     ]
    }
   ],
   "source": [
    "data_repos = glob.glob('../data/*')\n",
    "print(data_repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all filenames of each repo in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "repo_files_dict = defaultdict(list)\n",
    "for repo in data_repos:\n",
    "    filenames = glob.glob(repo + '/**/*.py',recursive=True)\n",
    "    repo_files_dict[repo] = filenames\n",
    "print(len(repo_files_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pandas dataframe to store repo name and their python files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "repos_df = defaultdict(list)\n",
    "for repo, filenames in repo_files_dict.items():\n",
    "    code = ''\n",
    "    for filename in filenames:\n",
    "        code += filename + '\\n'\n",
    "        file = open(filename,'r')\n",
    "        code += file.read()\n",
    "    repos_df['repo_name'].append(repo)\n",
    "    repos_df['code'].append(code)\n",
    "\n",
    "repos_df = pd.DataFrame.from_dict(repos_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary for Encoder\n",
    "### Refer https://arxiv.org/pdf/1508.07909.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/Horizon/setup.py\\n#!/usr/bin/env python3\\n# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\\n\\nfrom setuptools import find_packages, setup\\n\\n\\ndef readme():\\n    with open(\"README.md\") as f:\\n        return f.read()\\n\\n\\ndef requirements():\\n    with open(\"requirements.txt\") as f:\\n        return f.read()\\n\\n\\nsetup(\\n    name=\"horizon\",\\n    version=\"0.1\",\\n    author=\"Facebook\",\\n    description=(\"Facebook RL\"),\\n    long_description=readme(),\\n    url=\"https://github.com/facebookresearch/Horizon\",\\n    license=\"BSD\",\\n    packages=find_packages(),\\n    install_requires=[],\\n    dependency_links=[],\\n    test_suite=\"ml.rl.test\",\\n)\\n../data/Horizon/__init__.py\\n../data/Horizon/ml/__init__.py\\n../data/Horizon/ml/rl/__init__.py\\n#!/usr/bin/env python3\\n# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\\n../data/Horizon/ml/rl/tensorboardX.py\\n#!/usr/bin/env python3\\n# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\\n\\n\"\"\"\\nContext library to allow dropp'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ' <\\eol>\\n'.join(repos_df['code'])\n",
    "corpus[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(40000, pct_bpe=0.88,ngram_min=1,ngram_max=5)\n",
    "encoder.fit(corpus.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', '__sow', 'tri', 'm_v', 'ocab', '__eow', '(', 'n', ',', 'vocab', '):', '#', 'type', ':', '(', 'int', ',', 'dict', '[', 'str', ',', 'int', '])', '->', 'none', \"'''\", '__sow', 'delet', 'es', '__eow', 'all', '__sow', 'pairs', '__eow', 'below', '10', '*', 'vocab', 'size', 'to', 'prevent', 'memory', '__sow', 'probl', 'ems', '__eow', \"'''\", '__sow', 'pair', '_coun', 'ts', '__eow', '=', 'sorted', '(', 'vocab', '.', 'items', '(),', 'key', '=', 'lambda', 'p', ':', '-', 'p', '[', '1', '])', '__sow', 'pairs', '_to_t', 'rim', '__eow', '=', '[', 'pair', 'for', 'pair', ',', 'count', 'in', '__sow', 'pair', '_coun', 'ts', '__eow', '[', 'n', '__sow', ':]]', '__eow', 'for', 'pair', 'in', '__sow', 'pairs', '_to_t', 'rim', '__eow', ':', 'del', 'vocab', '[', 'pair', ']']\n",
      "[28, 4801, 5162, 15714, 9626, 4800, 5, 190, 3, 673, 20, 7, 135, 9, 5, 115, 3, 454, 12, 164, 3, 115, 73, 270, 45, 794, 4801, 37959, 4838, 4800, 114, 4801, 16681, 4800, 763, 222, 43, 673, 161, 27, 2988, 758, 4801, 36755, 15416, 4800, 794, 4801, 2547, 8768, 4924, 4800, 4, 1389, 5, 673, 2, 631, 266, 642, 4, 1978, 349, 9, 14, 349, 12, 11, 73, 4801, 16681, 13515, 18373, 4800, 4, 12, 2547, 19, 2547, 3, 401, 22, 4801, 2547, 8768, 4924, 4800, 12, 190, 4801, 21887, 4800, 19, 2547, 22, 4801, 16681, 13515, 18373, 4800, 9, 2039, 673, 12, 2547, 17]\n",
      "def trim_vocab ( n , vocab ): # type : ( int , dict [ str , int ]) -> none ''' deletes all pairs below 10 * vocab size to prevent memory problems ''' pair_counts = sorted ( vocab . items (), key = lambda p : - p [ 1 ]) pairs_to_trim = [ pair for pair , count in pair_counts [ n :]] for pair in pairs_to_trim : del vocab [ pair ]\n"
     ]
    }
   ],
   "source": [
    "example = \"def trim_vocab(n, vocab):\\n# type: (int, Dict[str, int]) -> None\\n''' Deletes all pairs below 10 * vocab size to prevent memory problems '''\\npair_counts = sorted(vocab.items(), key=lambda p: -p[1])\\npairs_to_trim = [pair for pair, count in pair_counts[n:]]\\nfor pair in pairs_to_trim:\\ndel vocab[pair]\"\n",
    "print(encoder.tokenize(example))\n",
    "print(next(encoder.transform([example])))\n",
    "print(next(encoder.inverse_transform(encoder.transform([example]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35200\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder.bpe_vocab))\n",
    "encoder.save('../model/bpe.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
