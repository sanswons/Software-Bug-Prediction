{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/Horizon', '../data/TensorFlow-Examples', '../data/text_classification', '../data/Detectron', '../data/neural-style']\n"
     ]
    }
   ],
   "source": [
    "data_repos = glob.glob('../data/*')\n",
    "print(data_repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all filenames of each repo in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "repo_files_dict = defaultdict(list)\n",
    "for repo in data_repos:\n",
    "    filenames = glob.glob(repo + '/**/*.py',recursive=True)\n",
    "    repo_files_dict[repo] = filenames\n",
    "print(len(repo_files_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pandas dataframe to store repo name and their python files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "repos_df = defaultdict(list)\n",
    "for repo, filenames in repo_files_dict.items():\n",
    "    code = ''\n",
    "    for filename in filenames:\n",
    "        code += filename + '\\n'\n",
    "        file = open(filename,'r')\n",
    "        code += file.read()\n",
    "    repos_df['repo_name'].append(repo)\n",
    "    repos_df['code'].append(code)\n",
    "\n",
    "repos_df = pd.DataFrame.from_dict(repos_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary for Encoder\n",
    "### Refer https://arxiv.org/pdf/1508.07909.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import encoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/text_classification/a08_predict_ensemble.py\\n# -*- coding: utf-8 -*-\\n#prediction using multi-models. take out: create multiple graphs. each graph associate with a session. add logits of models.\\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\\nimport sys\\nreload(sys)\\nsys.setdefaultencoding(\\'utf8\\')\\nimport tensorflow as tf\\nimport numpy as np\\nimport os\\nfrom a3_entity_network import EntityNetwork\\nsys.path.append(\"..\")\\nfrom a08_DynamicMemoryNetwork.data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\\nfrom tflearn.data_utils import pad_sequences #to_categorical\\nimport codecs\\nfrom a08_DynamicMemoryNetwork.a8_dynamic_memory_network import DynamicMemoryNetwork\\nfrom p7_TextCNN_model import TextCNN\\nfrom p71_TextRCNN_mode2 import TextRCNN\\n\\n#configuration\\nFLAGS=tf.app.flags.FLAGS\\ntf.app.flags.DEFINE_integer(\"num_classes\",1999,\"number of label\")\\ntf.app.flags.DEFINE_float(\"learning_rate\",0.01,\"learning rate\")'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ' <\\eol>\\n'.join(repos_df['code'])\n",
    "corpus[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(40000, pct_bpe=0.88,ngram_min=1,ngram_max=5)\n",
    "encoder.fit(corpus.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', '__sow', 'tri', 'm_v', 'ocab', '__eow', '(', 'n', ',', 'vocab', '):', '#', 'type', ':', '(', 'int', ',', 'dict', '[', 'str', ',', 'int', '])', '->', 'none', \"'''\", '__sow', 'dele', 'tes', '__eow', 'all', '__sow', 'pairs', '__eow', 'below', '10', '*', 'vocab', 'size', 'to', 'prevent', 'memory', '__sow', 'probl', 'ems', '__eow', \"'''\", '__sow', 'pair', '_coun', 'ts', '__eow', '=', 'sorted', '(', 'vocab', '.', 'items', '(),', 'key', '=', 'lambda', 'p', ':', '-', 'p', '[', '1', '])', '__sow', 'pairs', '_to_t', 'rim', '__eow', '=', '[', 'pair', 'for', 'pair', ',', 'count', 'in', '__sow', 'pair', '_coun', 'ts', '__eow', '[', 'n', '__sow', ':]]', '__eow', 'for', 'pair', 'in', '__sow', 'pairs', '_to_t', 'rim', '__eow', ':', 'del', 'vocab', '[', 'pair', ']']\n",
      "[28, 4801, 5167, 15844, 9580, 4800, 5, 190, 3, 673, 20, 7, 136, 9, 5, 115, 3, 452, 12, 164, 3, 115, 73, 270, 45, 797, 4801, 34150, 4975, 4800, 114, 4801, 16229, 4800, 762, 222, 43, 673, 161, 27, 2834, 759, 4801, 39589, 15808, 4800, 797, 4801, 2564, 8657, 4924, 4800, 4, 1381, 5, 673, 2, 629, 266, 638, 4, 2021, 349, 9, 14, 349, 12, 11, 73, 4801, 16229, 13518, 18584, 4800, 4, 12, 2564, 19, 2564, 3, 399, 22, 4801, 2564, 8657, 4924, 4800, 12, 190, 4801, 22676, 4800, 19, 2564, 22, 4801, 16229, 13518, 18584, 4800, 9, 2018, 673, 12, 2564, 17]\n",
      "def trim_vocab ( n , vocab ): # type : ( int , dict [ str , int ]) -> none ''' deletes all pairs below 10 * vocab size to prevent memory problems ''' pair_counts = sorted ( vocab . items (), key = lambda p : - p [ 1 ]) pairs_to_trim = [ pair for pair , count in pair_counts [ n :]] for pair in pairs_to_trim : del vocab [ pair ]\n"
     ]
    }
   ],
   "source": [
    "example = \"def trim_vocab(n, vocab):\\n# type: (int, Dict[str, int]) -> None\\n''' Deletes all pairs below 10 * vocab size to prevent memory problems '''\\npair_counts = sorted(vocab.items(), key=lambda p: -p[1])\\npairs_to_trim = [pair for pair, count in pair_counts[n:]]\\nfor pair in pairs_to_trim:\\ndel vocab[pair]\"\n",
    "print(encoder.tokenize(example))\n",
    "print(next(encoder.transform([example])))\n",
    "print(next(encoder.inverse_transform(encoder.transform([example]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35200\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder.bpe_vocab))\n",
    "encoder.save('../model/bpe.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
